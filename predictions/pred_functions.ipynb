{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a working directory\n",
    "#!pip install GitPython\n",
    "import git\n",
    "import os\n",
    "\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "\n",
    "\n",
    "os.chdir(repo.working_tree_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_single_population(covari_population):\n",
    "    \"\"\"\n",
    "    Takes the covari dataframe, and whichever of the four populations and returns a dataframe that\n",
    "    only includes the selected population's rows, a list of labels (biomass values associated with the dataframe)\n",
    "    and a list of all of the features.\n",
    "    \"\"\"\n",
    "    # Selecting the population based on the provided name\n",
    "    pop_df = covari_population\n",
    "    \n",
    "    # pop_df.drop(columns='hours_since_sunrise', inplace=True)\n",
    "\n",
    "    # Creating the labels and features for the population\n",
    "    labels = np.array(pop_df.biomass, copy=True)\n",
    "    labels = np.delete(labels, 0, 0)\n",
    "    features = pop_df.drop(['time', 'biomass', 'lat', 'lon', 'cruise'], axis=1, inplace=False)\n",
    "    # Saving feature names for later use\n",
    "    feature_list = list(features.columns)\n",
    "    features = features.to_numpy()\n",
    "    features = np.delete(features, 0, 0)\n",
    "    return pop_df, labels, features, feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "covari_path = 'data_ingest/data/modified/RF_ready_covari.csv'\n",
    "#using pandas to read in as a df\n",
    "covari = (pd.read_csv(covari_path,parse_dates=[0]))\n",
    "#taking a peak at the data\n",
    "covari.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covari_pro = covari.drop(columns=['biomass_pico','biomass_croco','biomass_syn'])\n",
    "covari_syn = covari.drop(columns=['biomass_pico','biomass_croco','biomass_pro'])\n",
    "covari_pico = covari.drop(columns=['biomass_syn','biomass_croco','biomass_pro'])\n",
    "covari_croco = covari.drop(columns=['biomass_pico','biomass_syn','biomass_pro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_dfer(covari_pop, pop_name):\n",
    "    \"\"\"\n",
    "    This function removes population names from the columns of each df\n",
    "    \"\"\"\n",
    "    df = covari_pop\n",
    "    pop_name = pop_name\n",
    "    df.rename(columns=lambda x: x.replace('_'+pop_name, ''), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_dfer(covari_syn, 'syn')\n",
    "population_dfer(covari_pro, 'pro')\n",
    "population_dfer(covari_pico, 'pico')\n",
    "population_dfer(covari_croco, 'croco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_df, labels_pro, features_pro, feature_list_pro = preprocess_single_population(covari_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of splits for k-fold cross-validation\n",
    "n_splits = 8\n",
    "kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "# Initialize lists to hold training and testing data\n",
    "train_features = []\n",
    "test_features = []\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "# Split the data into training and testing sets for each fold\n",
    "for train_index, test_index in kf.split(features_pro):\n",
    "    train_feat, test_feat = features_pro[train_index], features_pro[test_index]\n",
    "    train_lab, test_lab = labels_pro[train_index], labels_pro[test_index]\n",
    "    \n",
    "    # Append the training and testing data for this fold to the lists\n",
    "    train_features.append(train_feat)\n",
    "    test_features.append(test_feat)\n",
    "    train_labels.append(train_lab)\n",
    "    test_labels.append(test_lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = [0,1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(data):\n",
    "    # Find the length of the shortest sublist\n",
    "    min_len = min(len(sublist) for sublist in data)\n",
    "\n",
    "    # Remove data points from each sublist until they are all the same length\n",
    "    data_truncated = [sublist[:min_len] for sublist in data]\n",
    "\n",
    "    return data_truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = equalize(train_features)\n",
    "test_features = equalize(test_features)\n",
    "train_labels = equalize(train_labels)\n",
    "test_labels = equalize(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "lengths = [len(sublist) for sublist in train_features]\n",
    "print(set(lengths))\n",
    "print(type(train_features))\n",
    "print(type(train_labels))\n",
    "print(features_pro)\n",
    "train_features = numpy.array(train_features)\n",
    "train_labels = numpy.array(train_labels)\n",
    "test_features = numpy.array(test_features)\n",
    "test_labels = numpy.array(test_labels)\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using a regressor RF model because we are predicting on continous values\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# Initialize a list to hold the models for each fold\n",
    "models = []\n",
    "\n",
    "# Loop over the folds\n",
    "for i in range(train_features.shape[0]):\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators = 200, max_features='sqrt', max_depth = 10, random_state = 33)\n",
    "    \n",
    "   \n",
    "    rf.fit(train_features[i], train_labels[i])\n",
    "    \n",
    "    \n",
    "    models.append(rf)\n",
    "\n",
    "# Save the models\n",
    "for i, model in enumerate(models):\n",
    "    joblib.dump(model, f\"RF_models/pro_random_forest_fold_{i}.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
